{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rgCo_7-6QTX-"
      },
      "outputs": [],
      "source": [
        "!pip install requests pymupdf openai langchain langchain_community faiss-cpu tiktoken > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import openai\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from google.colab import userdata\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# PDF 텍스트 추출 함수 (파일 경로 사용)\n",
        "def extract_text_from_pdf(file_path):\n",
        "    # PyMuPDF로 PDF 파일 열기\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()  # 텍스트 추출\n",
        "    return text"
      ],
      "metadata": {
        "id": "8NvQgcygxkIx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. RAG 모델 설정"
      ],
      "metadata": {
        "id": "Gw1G0A0ix4W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4 API 키 설정\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 텍스트 분할기 설정\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)  # 텍스트 크기 및 중복 설정\n",
        "\n",
        "# 로컬 PDF 파일 경로\n",
        "pdf_file_path = '/content/2310.06825v1-1-6.pdf'  # 여기서 경로를 적절히 변경하십시오.\n",
        "\n",
        "# PDF에서 텍스트 추출\n",
        "document_text = extract_text_from_pdf(pdf_file_path)"
      ],
      "metadata": {
        "id": "vU_pFFMV1Zjy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트를 작은 덩어리로 나누기\n",
        "chunks = text_splitter.split_text(document_text)\n",
        "\n",
        "# 각 텍스트 덩어리를 Document 객체로 래핑\n",
        "documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "# 임베딩 및 FAISS 벡터화\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(documents, embedding)"
      ],
      "metadata": {
        "id": "oy5cvxuy2zD6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfwlcWDoQZqt",
        "outputId": "e7d4875c-e602-415a-f51e-11322285c5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-20384ce4bfb4>:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper introduces Mistral 7B, a 7-billion-parameter language model designed for superior performance and efficiency. The model outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Mistral 7B uses grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length with reduced inference cost. The model also includes a version fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model on both human and automated benchmarks. The models are released under the Apache 2.0 license.\n"
          ]
        }
      ],
      "source": [
        "# 최신 방식의 Chat 모델\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        ")\n",
        "\n",
        "query = \"Please summarize the main findings of this paper.\"\n",
        "summary = qa_chain.run(query)\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}