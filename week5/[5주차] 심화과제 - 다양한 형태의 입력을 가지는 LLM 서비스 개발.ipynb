{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rgCo_7-6QTX-"
      },
      "outputs": [],
      "source": [
        "!pip install requests pymupdf openai langchain langchain_community faiss-cpu tiktoken > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import openai\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from google.colab import userdata\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# PDF 텍스트 추출 함수 (파일 경로 사용)\n",
        "def extract_text_from_pdf(file_path):\n",
        "    # PyMuPDF로 PDF 파일 열기\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()  # 텍스트 추출\n",
        "    return text"
      ],
      "metadata": {
        "id": "8NvQgcygxkIx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. RAG 모델 설정"
      ],
      "metadata": {
        "id": "Gw1G0A0ix4W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4 API 키 설정\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 텍스트 분할기 설정\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)  # 텍스트 크기 및 중복 설정\n",
        "\n",
        "# 로컬 PDF 파일 경로\n",
        "pdf_file_path = '/content/2310.06825v1-1-6.pdf'  # 여기서 경로를 적절히 변경하십시오.\n",
        "\n",
        "# PDF에서 텍스트 추출\n",
        "document_text = extract_text_from_pdf(pdf_file_path)"
      ],
      "metadata": {
        "id": "vU_pFFMV1Zjy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트를 작은 덩어리로 나누기\n",
        "chunks = text_splitter.split_text(document_text)\n",
        "\n",
        "# 각 텍스트 덩어리를 Document 객체로 래핑\n",
        "documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "# 임베딩 및 FAISS 벡터화\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(documents, embedding)"
      ],
      "metadata": {
        "id": "oy5cvxuy2zD6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfwlcWDoQZqt",
        "outputId": "8aea75b2-1c84-4dc2-b286-fb9371915f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper introduces Mistral 7B, a 7-billion-parameter language model designed for high performance and efficiency in natural language processing. It outperforms the best open 13B model (Llama 2) across all benchmarks and surpasses the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Mistral 7B employs innovative techniques such as grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle longer sequences efficiently. The model is fine-tuned to follow instructions, resulting in Mistral 7B – Instruct, which outperforms Llama 2 13B in human and automated benchmarks. The findings highlight that Mistral 7B achieves high performance while maintaining efficiency, suggesting a new approach to model design that balances capability and computational cost.\n"
          ]
        }
      ],
      "source": [
        "# 최신 방식의 Chat 모델\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        ")\n",
        "\n",
        "query = (\n",
        "    \"You are a helpful AI assistant tasked with summarizing a scientific paper. \"\n",
        "    \"Summarize the main contributions, methods, and findings of the following paper \"\n",
        "    \"in a concise paragraph.\"\n",
        ")\n",
        "summary = qa_chain.run(query)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P5zei-cS7uE7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}